{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d95d6b",
   "metadata": {},
   "source": [
    "# Results: Extra Analysis\n",
    "Analysis that are not included in the other results notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84d610",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pandas import read_csv, DataFrame, read_excel\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, Dropdown, IntSlider\n",
    "import random\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import join, splitext, isfile\n",
    "\n",
    "from nft_helpers.utils import (\n",
    "    load_yaml, imread, im_to_txt_path, get_filename, imwrite\n",
    ")\n",
    "from nft_helpers.girder_dsa import login, get_tile_metadata\n",
    "from nft_helpers.yolov5.utils import read_yolo_label\n",
    "from nft_helpers.roi_utils import read_roi_txt_file, line_to_xys\n",
    "\n",
    "cf = load_yaml()\n",
    "\n",
    "# Create a directory to save files to.\n",
    "save_dir = join(cf.datadir, 'results/extras')\n",
    "makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2515bf5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Authenticate client.\n",
    "gc = login(join(cf.dsaURL, 'api/v1'), username=cf.user, password=cf.password)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395ab30",
   "metadata": {},
   "source": [
    "## Cohort Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345730e5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create the table as a dataframe.\n",
    "cases_df = read_csv('csvs/cases.csv').fillna('')  # case metadata\n",
    "\n",
    "# split the cases into each cohort\n",
    "cohorts = ['Inference-Cohort-1', 'Inference-Cohort-2', 'External-Cohort']\n",
    "cohort_dfs = {cohort: cases_df[cases_df.cohort == cohort] for cohort in cohorts}\n",
    " \n",
    "cohorts_df = [['', '', 'Emory-Train', 'Emory-Test', 'UC Davis']]\n",
    "\n",
    "cohorts_df.append(['Demographics', '', '', '', ''])\n",
    "\n",
    "# Add the demographics sex row.\n",
    "row = ['', 'Number of cases (M/F)']\n",
    "\n",
    "for cohort in cohorts:\n",
    "    counts = cohort_dfs[cohort].sex.value_counts()\n",
    "        \n",
    "    f = counts['female'] if 'female' in counts else 0\n",
    "    m = counts['male'] if 'male' in counts else 0\n",
    "    \n",
    "    row.append(f'{counts.sum()} ({m}/{f})')\n",
    "    \n",
    "cohorts_df.append(row)\n",
    "\n",
    "# Add average age at death row.\n",
    "row = ['', 'Average age at death (standard deviation)']\n",
    "\n",
    "for cohort in cohorts:\n",
    "    age_at_death = cohort_dfs[cohort].age_at_death.replace('90+', 90).astype(int)\n",
    "    row.append(f'{age_at_death.mean():.2f} ({age_at_death.std():.2f})')\n",
    "    \n",
    "cohorts_df.append(row)\n",
    "\n",
    "# Add race info.\n",
    "cohorts_df.append(['', 'Race/Ethnicity:', '', '', ''])\n",
    "\n",
    "for race in ('Caucasian', 'Black / African American', 'Hispanic', 'Asian','unknown'):\n",
    "    row = ['', f'  {race}']\n",
    "    \n",
    "    for cohort in cohorts:\n",
    "        counts = cohort_dfs[cohort].race.value_counts()\n",
    "        \n",
    "        count = counts[race] if race in counts else 0\n",
    "        \n",
    "        if count:\n",
    "            row.append(f'{count} ({count / counts.sum() * 100:.2f}%)')\n",
    "        else:\n",
    "            row.append('-')\n",
    "            \n",
    "    cohorts_df.append(row)\n",
    "\n",
    "# Add the Braak Stage info.\n",
    "cohorts_df.append(['Braak NFT Stage', '', '', '', ''])\n",
    "\n",
    "stage_map = {\n",
    "    '0': '0', '1': 'I', '1-2': 'I-II', '2': 'II', '3': 'III', '4': 'IV', '5': 'V', '6': 'VI',\n",
    "}\n",
    "for stage, rstage in stage_map.items():\n",
    "    row = ['', rstage]\n",
    "    \n",
    "    for cohort in cohorts:\n",
    "        stages = cohort_dfs[cohort].Braak_stage.value_counts()\n",
    "        \n",
    "        count = int(stages.get(stage)) if stage in stages else 0\n",
    "        \n",
    "        if count:\n",
    "            row.append(f'{count} ({count / stages.sum() * 100:.2f}%)')\n",
    "        else:\n",
    "            row.append('-')\n",
    "        \n",
    "    cohorts_df.append(row)\n",
    "    \n",
    "# Add antibody distribution.\n",
    "cohorts_df.append(['Tau Antibody (WSI counts)', '', '', '', ''])\n",
    "\n",
    "ab_df = read_csv('csvs/Tau antibody for Emory WSIs.csv')\n",
    "\n",
    "for ab in ['PHF-1', 'AT8', 'CP13', 'Accurate']:\n",
    "    row = ['', ab]\n",
    "    \n",
    "    for cohort in ['Emory-Train', 'Emory-Holdout']:\n",
    "        ab_cohort = ab_df[ab_df.Cohort == cohort]\n",
    "        ab_count = len(ab_cohort[ab_cohort['Antibody Info'] == ab])\n",
    "        \n",
    "        if ab_count:\n",
    "            row.append(f'{ab_count} ({ab_count / len(ab_cohort) * 100:.2f}%)')\n",
    "        else:\n",
    "            row.append('-')\n",
    "        \n",
    "    if ab == 'AT8':\n",
    "        row.append('92 (100%)')\n",
    "    else: \n",
    "        row.append('-')\n",
    "        \n",
    "    cohorts_df.append(row)\n",
    "    \n",
    "# Build into dataframe - save to file.\n",
    "cohorts_df = DataFrame(cohorts_df, \n",
    "                       columns=['', '', 'Cohorts', '', ''])\n",
    "cohorts_df.to_csv(join(save_dir, 'Cohort Information.csv'), index=False)\n",
    "cohorts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a50b01",
   "metadata": {},
   "source": [
    "## Example Pre-NFT & iNFT Annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238457af",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Examples of Pre-NFT / iNFT\n",
    "def rgb_to_rgba(img: np.array, alpha: int = 255) -> np.array:\n",
    "    \"\"\"Convert an RGB image to an RGBA image by adding an alpha channel.\n",
    "    \n",
    "    Args:\n",
    "        img: RGB image.\n",
    "        alpha: Alpha value to add to all pixels. 0 is transparent and 255 is\n",
    "            non-transparant.\n",
    "            \n",
    "    Returns:\n",
    "        Image with alpha channel.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Split image to R, G, and B channels.\n",
    "    b_channel, g_channel, r_channel = cv.split(img)\n",
    "    \n",
    "    # Create alpha channel\n",
    "    a_channel = np.ones(b_channel.shape, dtype=b_channel.dtype) * alpha\n",
    "    \n",
    "    # Merge the channels\n",
    "    return cv.merge((b_channel, g_channel, r_channel, a_channel))\n",
    "\n",
    "\n",
    "ann_df = read_csv('csvs/annotations.csv')\n",
    "ann_df = ann_df[ann_df.annotator == 'expert1']\n",
    "\n",
    "\n",
    "def select_nft_type(nft_type):\n",
    "    \"\"\"Select the NFT type, and then create the slider interactive.\"\"\"\n",
    "    nft_ann_df = ann_df[ann_df.label == nft_type]\n",
    "    color = (255, 0, 0, 255) if nft_type == 'iNFT' else (0, 0, 255, 255)\n",
    "    \n",
    "    \n",
    "    def show_image(i):\n",
    "        r = nft_ann_df.iloc[i]\n",
    "        \n",
    "        # Read the image with an alpha channel.\n",
    "        img = rgb_to_rgba(imread(r.im_path))\n",
    "        \n",
    "        # Draw the NFT box with transparency.\n",
    "        box = line_to_xys(r.box_coords) - [r.im_left, r.im_top]        \n",
    "        img = cv.rectangle(img, box[0,:], box[1,:], color, 2)\n",
    "        \n",
    "        img = img[150:350, 150:350, :]\n",
    "        \n",
    "        # Zoom in by taken a center box of the image.\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.savefig(\n",
    "            join(save_dir, f'{nft_type} sample.png'), \n",
    "            bbox_inches='tight', \n",
    "            dpi=300\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    _ = interact(\n",
    "        show_image, \n",
    "        i=IntSlider(\n",
    "            min=0, max=len(nft_ann_df)-1, continuous_update=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "_ = interact(select_nft_type, nft_type=Dropdown(options=['Pre-NFT', 'iNFT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b1b02",
   "metadata": {},
   "source": [
    "## Supplementary File 1: WSI Information.\n",
    "Include more detail on each case by providing info on all WSIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b229d3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Compile WSI metadata into a single dataframe / table.\n",
    "save_fp = 'csvs/Supplementary File 1.csv'\n",
    "\n",
    "if isfile(save_fp):\n",
    "    wsi_metadata = read_csv(save_fp)\n",
    "else:\n",
    "    wsis = read_csv('csvs/wsis.csv').fillna('')\n",
    "    cohorts = ['Inference-Cohort-1', 'Inference-Cohort-2', 'External-Cohort']\n",
    "    wsis = wsis[wsis.cohort.isin(cohorts)]\n",
    "\n",
    "    # Marla info on antibodies for Emory.\n",
    "    marla_ab_df = read_excel('csvs/Emory-antibody-info-MG.xlsx').fillna('')\n",
    "\n",
    "    cases = read_csv('csvs/cases.csv').fillna('')\n",
    "    cases = {r.case: r for _, r in cases.iterrows()}\n",
    "\n",
    "    # Read antibody info.\n",
    "    # ab_df = read_csv('csvs/wsis-antibody-info.csv').fillna('')\n",
    "    # ab_map = {r.Filename: r.Antibody for _, r in ab_df.iterrows() if r.Antibody in \\\n",
    "    #           ('PHF-1', 'AT8', 'CP13', 'ACC')}\n",
    "\n",
    "    wsi_metadata = []\n",
    "\n",
    "    # Exclude annotated cohort - same as the inference cohort 1\n",
    "    for _, r in tqdm(wsis.iterrows(), total=len(wsis)):\n",
    "        # Look for antibody.\n",
    "        ab_info = marla_ab_df[marla_ab_df.Filename == r.wsi_name]\n",
    "\n",
    "        if len(ab_info):\n",
    "            ab_info = ab_info.iloc[0]\n",
    "\n",
    "            if ab_info['Antibody Info']:\n",
    "                ab = ab_info['Antibody Info']\n",
    "            else:\n",
    "                ab = ab_info.Antibody\n",
    "        elif r.cohort == 'External-Cohort':\n",
    "            ab = 'AT8'\n",
    "        else:\n",
    "            raise Exception(f'Antibody info missing for {r.wsi_name}')\n",
    "\n",
    "        if r.cohort == 'Inference-Cohort-1':\n",
    "            cohort = 'Emory (Train)'\n",
    "        elif r.cohort == 'Inference-Cohort-2':\n",
    "            cohort = 'Emory (Test)'\n",
    "        else:\n",
    "            cohort = 'UC Davis'\n",
    "\n",
    "        ts = get_tile_metadata(gc, r.wsi_id)\n",
    "        sizeX, sizeY = ts['sizeX'], ts['sizeY']\n",
    "        mag = ts['magnification']\n",
    "\n",
    "        abc = int(case.ABC) if case.ABC != '' else ''\n",
    "        thal = int(case.Thal) if case.Thal != '' else ''\n",
    "\n",
    "        case = cases[r.case]\n",
    "\n",
    "        wsi_metadata.append([\n",
    "            cohort, r.case, r.wsi_name, r.region, mag, f'{sizeX} x {sizeY}', \n",
    "            splitext(r.wsi_name)[-1][1:].upper(), case.Braak_stage, thal,\n",
    "            abc, case.age_at_death, case.race, case.sex, case.Clinical_Dx,\n",
    "            case.Primary_NP_Dx, ab, ts['mm_x'] * 1000, ts['mm_y'] * 1000\n",
    "        ])\n",
    "\n",
    "    wsi_metadata = DataFrame(\n",
    "        wsi_metadata, \n",
    "        columns=[\n",
    "            'cohort', 'Case ID', 'WSI Name', 'Brain Region', \n",
    "            'Scanned Magnification', 'Height x width in pixels', 'File Type', \n",
    "            'Braak NFT Stage', 'Thal phase', 'ABC score', 'Age at Death', \n",
    "            'Race/Ethnicity', 'Sex', 'Clinical Diagnosis', \n",
    "            'Primary Neuropathology Diagnosis', 'Tau Antibody', \n",
    "            'microns / pixel (horizontal)', 'microns / pixel (vertical)'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    wsi_metadata['Tau Antibody'] = wsi_metadata['Tau Antibody'].replace(\n",
    "        {'likely PHF-1': 'PHF-1'}\n",
    "    )\n",
    "    \n",
    "    wsi_metadata.to_csv(save_fp, index=False)\n",
    "    \n",
    "print('Sample data:')\n",
    "display(wsi_metadata.sample(n=5))\n",
    "\n",
    "# Report the resolution for each cohort.\n",
    "print(\n",
    "    '\\nAverages (with standard deviations) of resolutions by cohort in ' + \\\n",
    "    'microns per pixel:'\n",
    ")\n",
    "for cohort in wsi_metadata.cohort.unique():\n",
    "    um_x = wsi_metadata[wsi_metadata.cohort == cohort][\n",
    "        'microns / pixel (vertical)'\n",
    "    ]\n",
    "    um_y = wsi_metadata[wsi_metadata.cohort == cohort][\n",
    "        'microns / pixel (horizontal)'\n",
    "    ]\n",
    "    \n",
    "    print(f'   Cohort {cohort}: {um_x.mean():.2f} x {um_y.mean():.2f} ' + \\\n",
    "          f'(± {um_x.std():.2f} x {um_y.std():.2f})')\n",
    "    \n",
    "# Report for all and also for the cohort\n",
    "print('\\nAntibodies in Emory Cohorts:')\n",
    "\n",
    "for cohorts in ([\n",
    "    'Emory (Train)', 'Emory (Test)'], 'Emory (Train)', 'Emory (Test)'\n",
    "):\n",
    "    if isinstance(cohorts, str):\n",
    "        cohorts = [cohorts]\n",
    "        \n",
    "    counts = wsi_metadata[\n",
    "        wsi_metadata.cohort.isin(cohorts)\n",
    "    ]['Tau Antibody'].value_counts()\n",
    "    \n",
    "    print(f'  Cohorts: {cohorts}')\n",
    "    \n",
    "    for k, v in counts.items():\n",
    "        print(f'    {k} (n={v})')\n",
    "    print(f'  Total N={counts.sum()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252fc8df",
   "metadata": {},
   "source": [
    "## Average Size of ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210e36d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Read the ROI data and subset to only the inter-annotator ones.\n",
    "rois_df = read_csv('csvs/rois.csv')\n",
    "\n",
    "iaa_df = rois_df[\n",
    "    (rois_df.cohort == 'Annotated-Cohort') & (rois_df.annotator == 'expert1') \\\n",
    "    & (rois_df.roi_group == 'ROIv2')\n",
    "]\n",
    "\n",
    "iaa_df = rois_df[\n",
    "    (rois_df.cohort == 'Annotated-Cohort') \\\n",
    "    & (rois_df.roi_group == 'ROIv3')\n",
    "]\n",
    "\n",
    "# size_arr = []\n",
    "# sizes = ''\n",
    "\n",
    "# for _, r in tqdm(iaa_df.iterrows():\n",
    "#     tile_metadata = get_tile_metadata(gc, r.wsi_id)\n",
    "    \n",
    "#     # Convert the ROI width and height to millemters\n",
    "#     w, h = r.roi_width, r.roi_height\n",
    "    \n",
    "#     # Get the scale factor\n",
    "#     w = w * tile_metadata['mm_x'] * 1000 # to microns\n",
    "#     h = h * tile_metadata['mm_y'] * 1000\n",
    "    \n",
    "#     w, h = sorted([w, h])\n",
    "    \n",
    "#     size_arr.append([w, h])\n",
    "#     sizes += f'{w:.0f}x{h:.0f}\\n'\n",
    "    \n",
    "# size_arr = np.array(size_arr)\n",
    "# w, h = np.mean(size_arr, axis=0)\n",
    "\n",
    "# print(f'Average size of ROI in microns: {w:.0f} x {h:.0f} microns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b980c02",
   "metadata": {},
   "source": [
    "## Tiling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd36e2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Read ROI info.\n",
    "tiles_df = read_csv('/workspace/data/datasets/annotator-datasets/tiles.csv')\n",
    "\n",
    "# Grab a single ROI.\n",
    "roi_info = tiles_df.iloc[0]\n",
    "roi_fp = roi_info.roi_fp\n",
    "\n",
    "# Subset tiles to only this roi\n",
    "tiles_df = tiles_df[tiles_df.roi_fp == roi_fp]\n",
    "\n",
    "# Read the ROI Image and save with boxes.\n",
    "roi_img = imread(roi_fp)\n",
    "\n",
    "for box in read_roi_txt_file(im_to_txt_path(roi_fp)):\n",
    "    lb, x1, y1, x2, y2 = box\n",
    "    roi_img = cv.rectangle(\n",
    "        roi_img, \n",
    "        (x1, y1), \n",
    "        (x2, y2), \n",
    "        (255, 0, 0) if lb else (0, 0, 255), \n",
    "        10\n",
    "    )\n",
    "    \n",
    "plt.figure(figsize=(5,10))\n",
    "plt.imshow(roi_img)\n",
    "plt.axis('off')\n",
    "plt.savefig(join(save_dir, f'sample-roi-with-labels.png'), \n",
    "            bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Draw overlapping grids\n",
    "for x in np.arange(0, roi_img.shape[1], 960):\n",
    "    for y in np.arange(0, roi_img.shape[0], 960):\n",
    "        roi_img = cv.rectangle(roi_img, (x, y), (x+1280, y+1280), (0, 0, 0), 10)\n",
    "        \n",
    "plt.figure(figsize=(5,10))\n",
    "plt.imshow(roi_img)\n",
    "plt.axis('off')\n",
    "plt.savefig(join(save_dir, f'sample-roi-with-grids.png'), \n",
    "            bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Save a random tile image with labels.\n",
    "tile_info = tiles_df.sample(n=1, random_state=64).iloc[0]\n",
    "\n",
    "tile_img = imread(tile_info.fp)\n",
    "\n",
    "for box in read_yolo_label(\n",
    "    im_to_txt_path(tile_info.fp), im_shape=(1280, 1280), convert=True\n",
    "):\n",
    "    lb, x1, y1, x2, y2 = box.astype(int)\n",
    "    \n",
    "    tile_img = cv.rectangle(\n",
    "        tile_img, (x1, y1), (x2, y2), (255, 0, 0) if lb else (0, 0, 255), 10\n",
    "    )\n",
    "    \n",
    "plt.imshow(tile_img)\n",
    "plt.axis('off')\n",
    "plt.savefig(join(save_dir, f'sample-tile-with-labels.png'), \n",
    "            bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ff6ab",
   "metadata": {},
   "source": [
    "## Consensus Labeling\n",
    "Choose a single ROI from the consensus labeling set. Draw the labels created from different *n* consensus values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad707e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI info - choose a single ROI to show that has NFTs and is not rotated.\n",
    "dataset_dir = join(cf.datadir, 'datasets/model-assisted-labeling')\n",
    "rois_df = read_csv(join(dataset_dir, 'rois.csv'))\n",
    "\n",
    "rois_df = rois_df[rois_df.wsi_id == '638147727f8a5e686a53837b']\n",
    "\n",
    "raw_img = imread(roi_meta.fp)\n",
    "h, w = raw_img.shape[:2]\n",
    "\n",
    "fn = get_filename(roi_meta.fp) + '.txt'\n",
    "\n",
    "# Get the large image metadata to draw scale bar.\n",
    "ts = get_tile_metadata(gc, roi_meta.wsi_id)\n",
    "\n",
    "# Get the pixels for 200 microns.\n",
    "px = int(200 / (ts['mm_x'] * 1000))\n",
    "\n",
    "# Draw scale bar.\n",
    "roi_img = cv.line(raw_img.copy(), (w-px, h-100), (w, h-100), (0, 0, 0), 30)\n",
    "\n",
    "# imwrite(join(save_dir, 'sample-roi.png'), roi_img)\n",
    "\n",
    "# Draw the prediction of each expert / novice model on this ROI.\n",
    "for model in ('expert1', 'expert2', 'expert3', 'expert4', 'expert5', 'novice1',\n",
    "              'novice2', 'novice3'):\n",
    "    # Read the prediction file.\n",
    "    model_roi = raw_img.copy()\n",
    "    \n",
    "    for box in read_yolo_label(\n",
    "        join(dataset_dir, 'rois/predictions', model, fn), \n",
    "        im_shape=(w, h), \n",
    "        convert=True\n",
    "    ):\n",
    "        label, x1, y1, x2, y2 = box[:5].astype(int)\n",
    "        \n",
    "        xc, yc = int((x2 + x1) / 2), int((y2 + y1) / 2)\n",
    "        \n",
    "        color = (255, 0, 0) if label else (0, 0, 255)\n",
    "        \n",
    "        model_roi = cv.circle(model_roi, (xc, yc), 100, color, 40)\n",
    "\n",
    "#     imwrite(join(save_dir, f'sample-roi-{model}-predictions.png'), model_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0f4b2",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw consensus images - 1, 4, and 8\n",
    "pad = 75\n",
    "cls = 1\n",
    "i = 2\n",
    "\n",
    "for model in ('1', '4', '8'):\n",
    "    model_roi = roi_img.copy()\n",
    "    model_box = raw_img.copy()\n",
    "    \n",
    "    boxes = read_yolo_label(\n",
    "        join(dataset_dir, 'rois/consensus', model, fn),\n",
    "        im_shape=(w, h),\n",
    "        convert=True\n",
    "    )\n",
    "    \n",
    "    box_dict = {0: [], 1: []}\n",
    "    \n",
    "    for box in boxes:\n",
    "        label, x1, y1, x2, y2 = box[:5].astype(int)\n",
    "        \n",
    "        xc, yc = int((x2 + x1) / 2), int((y2 + y1) / 2)\n",
    "        \n",
    "        box_dict[label].append((xc, yc))\n",
    "        \n",
    "        color = (255, 0, 0) if label else (0, 0, 255)\n",
    "        \n",
    "        model_roi = cv.circle(model_roi, (xc, yc), 100, color, 40)\n",
    "        model_box = cv.rectangle(model_box, (x1, y1), (x2, y2), color, 3)\n",
    "#     imwrite(\n",
    "#         join(save_dir, f'sample-roi-consensus-{model}-predictions.png'), \n",
    "#         model_roi\n",
    "#     )\n",
    "    \n",
    "    if i >= len(box_dict[cls]):\n",
    "        continue\n",
    "        \n",
    "    xc, yc = box_dict[cls][i]\n",
    "    \n",
    "    x1, y1, x2, y2 = xc - pad, yc - pad, xc + pad, yc + pad\n",
    "\n",
    "    if x1 < 0:\n",
    "        x1, x2 = 0, pad*2\n",
    "        \n",
    "    if y1 < 0:\n",
    "        y1, y2 = 0, pad*2\n",
    "        \n",
    "    if x2 > w:\n",
    "        x1, x2 = w-(pad*2), w\n",
    "        \n",
    "    if y2 > h:\n",
    "        y1, y2 = h-(pad*2), h\n",
    "        \n",
    "    img = model_box[y1:y2, x1:x2]\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Consensus n={model}', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#     imwrite(join(save_dir, f'sample-roi-iNFT-consensus-{model}.png'), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4c0bd",
   "metadata": {},
   "source": [
    "## Emory NFT Predictions by Antibody (AT8 vs PHF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe3d63",
   "metadata": {
    "code_folding": [
     21
    ]
   },
   "outputs": [],
   "source": [
    "# Read the imaging features for each case.\n",
    "fts_df = read_csv(join(cf.datadir, \n",
    "                       'results/wsi-inference/inference-features.csv'))\n",
    "fts_df = fts_df[fts_df.dataset.isin(('train', 'Emory test'))]\n",
    "\n",
    "# Create a map to know for each case which regions are stained with AT8 or PHF1.\n",
    "ab_df = read_csv('csvs/Tau antibody for Emory WSIs.csv')\n",
    "\n",
    "# Add region to each WSI.\n",
    "wsis_df = read_csv('csvs/wsis.csv')\n",
    "\n",
    "for i, r in ab_df.iterrows():\n",
    "    region = wsis_df[wsis_df.wsi_name == r.Filename].iloc[0].region\n",
    "    \n",
    "    ab_df.loc[i, 'region'] = region\n",
    "    \n",
    "ab_df = ab_df.replace({\n",
    "    'Right hippocampus': 'Hippocampus', 'Left hippocampus': 'Hippocampus'\n",
    "})\n",
    "\n",
    "\n",
    "def plot_ab_by_regions(region):\n",
    "    plot_df = []\n",
    "\n",
    "    for _, r in fts_df.iterrows():\n",
    "        # Get the antibody for this region and case.\n",
    "        ab = ab_df[\n",
    "            (ab_df.Case == r.case) & (ab_df.region == region)\n",
    "        ].iloc[0]['Antibody Info']\n",
    "\n",
    "        if ab not in ('AT8', 'PHF-1'):\n",
    "            continue\n",
    "\n",
    "        row = [r.dataset, r.case, r.stage, r.age, r.sex, 'Pre-NFT', ab,\n",
    "               r[f'Pre-NFT density ({region})']]\n",
    "        plot_df.append(row)\n",
    "\n",
    "        row = [r.dataset, r.case, r.stage, r.age, r.sex, 'iNFT', ab,\n",
    "               r[f'iNFT density ({region})']]\n",
    "        plot_df.append(row)\n",
    "\n",
    "\n",
    "    plot_df = DataFrame(\n",
    "        plot_df, \n",
    "        columns=[\n",
    "            'dataset', 'case', 'stage', 'age_at_death', 'sex', 'label', 'antibody',\n",
    "            f'density'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # For this region plot two suplots - one for each NFT class.\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    y_max = plot_df['density'].max()\n",
    "\n",
    "    for i, cls in enumerate(('Pre-NFT', 'iNFT')):\n",
    "        df = plot_df[plot_df.label == cls]\n",
    "\n",
    "        fig.add_subplot(1, 2, i+1)\n",
    "\n",
    "        ax = plt.gca()\n",
    "        sns.boxplot(data=df, y='density', x='stage', hue='antibody', ax=ax)\n",
    "\n",
    "        # Format figure.\n",
    "        plt.xlabel('Braak Stage', fontsize=16, fontweight='bold')\n",
    "        plt.ylabel('Density (Object / tissue area)', fontsize=16, fontweight='bold')\n",
    "        plt.xticks(fontweight='bold', fontsize=12)\n",
    "        plt.yticks(fontweight='bold', fontsize=12)\n",
    "        plt.title(cls, fontsize=16, fontweight='bold', y=1.15)\n",
    "        plt.legend(ncol=3, fontsize=14, bbox_to_anchor=(0.55, 1.15), \n",
    "                   loc='upper center')\n",
    "        plt.ylim([0, y_max])\n",
    "        ax.tick_params(axis='both', which='both', direction='out', length=10, \n",
    "                        width=2)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_linewidth(2)\n",
    "        ax.spines['left'].set_linewidth(2)\n",
    "\n",
    "    ax.get_legend().remove()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "_ = interact(\n",
    "    plot_ab_by_regions, \n",
    "    region=Dropdown(options=[\n",
    "        'Hippocampus', 'Amygdala', 'Temporal cortex', 'Occipital cortex'\n",
    "    ])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
