{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdcdeb6",
   "metadata": {},
   "source": [
    "# Results: Annotator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e50fe",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pandas import read_csv, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import join\n",
    "\n",
    "from nft_helpers import compile_model_results\n",
    "from nft_helpers.utils import load_yaml, imread\n",
    "from nft_helpers.interactive import model_bars, confusion_matrices\n",
    "from nft_helpers.box_and_contours import line_to_xys\n",
    "from nft_helpers.girder_dsa import login, get_tile_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36538bcc",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cf = load_yaml()\n",
    "save_dir = join(cf.datadir, 'results/annotator-models')\n",
    "makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "annotators = models = [\n",
    "    'novice1', 'novice2', 'novice3', 'expert1', 'expert2', 'expert3', 'expert4',\n",
    "    'expert5'\n",
    "]\n",
    "model_results = compile_model_results(join(cf.datadir, 'models'))\n",
    "\n",
    "kwargs = {  # plotting\n",
    "    'errorbar': 'se', 'edgecolor': 'k', 'lw': 3, 'width': 0.5, 'errcolor': 'k', \n",
    "    'errwidth': 3, 'capsize': 0.25\n",
    "}\n",
    "\n",
    "# Configuration (might delete this cell and moved to imports)\n",
    "results_dir = join(cf.datadir, 'results')\n",
    "model_results = compile_model_results(join(cf.datadir, 'models'))\n",
    "\n",
    "models = [\n",
    "    'novice1', 'novice2', 'novice3', 'expert1', 'expert2', 'expert3', 'expert4',\n",
    "    'expert5'\n",
    "]\n",
    "datasets = ['val', 'test', 'test-roi', 'test-external-roi']\n",
    "\n",
    "gc = login(join(cf.dsaURL, 'api/v1'), username=cf.user, password=cf.password)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41184b97",
   "metadata": {},
   "source": [
    "## Table: Annotations Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257ee48",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Annotations summary table.\n",
    "ann_df = read_csv('csvs/annotations.csv').fillna('')\n",
    "\n",
    "# Build the dataframe for the table.\n",
    "ann_summary_df = []\n",
    "\n",
    "for annotator in annotators:\n",
    "    if len(annotator) and annotator != 'novice4':\n",
    "        # Subet to annotations for this annotator.\n",
    "        annotator_df = ann_df[ann_df.annotator == annotator]\n",
    "        \n",
    "        # Add summary of annotations for this annotator.\n",
    "        class_counts = annotator_df.label.value_counts()\n",
    "        \n",
    "        ann_summary_df.append([\n",
    "            annotator,\n",
    "            class_counts['Pre-NFT'] if 'Pre-NFT' in class_counts else 0,\n",
    "            class_counts['iNFT'] if 'iNFT' in class_counts else 0,\n",
    "            len(annotator_df.roi_im_path.unique()),\n",
    "            len(annotator_df.wsi_name.unique()),\n",
    "            len(annotator_df.case.unique())\n",
    "        ])\n",
    "        \n",
    "ann_summary_df = DataFrame(\n",
    "    ann_summary_df, \n",
    "    columns=['Annotator', 'Pre-NFT\\nAnnotations', 'iNFT\\nAnnotations', '# ROIs',\n",
    "             '# WSIs', '# Cases']\n",
    ")\n",
    "\n",
    "ann_summary_df.to_csv(join(save_dir, 'annotations-summary.csv'), index=False)\n",
    "ann_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f264c5",
   "metadata": {},
   "source": [
    "## Table: Annotator Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe76ed",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Table showing model performance.\n",
    "perf_df = [\n",
    "    ['', 'Val', 'Test', 'Emory-Test', 'Val', 'Test', 'Emory-Test', 'Val', \n",
    "     'Test', 'Emory-Test']\n",
    "]\n",
    "\n",
    "for annotator in annotators:\n",
    "    ann_results = model_results[model_results.model == annotator]\n",
    "    \n",
    "    row = [annotator]\n",
    "    \n",
    "    test_df = ann_results[ann_results.dataset == 'test']\n",
    "    val_df = ann_results[ann_results.dataset == 'val']\n",
    "    em_df = ann_results[ann_results.dataset == 'test-roi']\n",
    "    \n",
    "    # Use lst as a hold-out parameter for adding mean +/ std. dev.\n",
    "    # Pre-NFT columns.\n",
    "    lst = val_df[val_df.label == 'Pre-NFT']['F1 score (Pre-NFT)']\n",
    "    row.append(f'{lst.mean():.2f}' + u' \\u00B1 ' + f'{lst.std():.2f}')\n",
    "    \n",
    "    lst = test_df[test_df.label == 'Pre-NFT']['F1 score (Pre-NFT)']\n",
    "    row.append(f'{lst.mean():.2f}' + u' \\u00B1 ' + f'{lst.std():.2f}')\n",
    "    \n",
    "    lst = em_df[em_df.label == 'Pre-NFT']['F1 score (Pre-NFT)']\n",
    "    row.append(f'{lst.mean():.2f}' + u' \\u00B1 ' + f'{lst.std():.2f}')\n",
    "    \n",
    "    # iNFT columns.\n",
    "    lst = val_df[val_df.label == 'iNFT']['F1 score (iNFT)']\n",
    "    row.append(f'{lst.mean():.2f}' + u' \\u00B1 ' + f'{lst.std():.2f}')\n",
    "    \n",
    "    lst = test_df[test_df.label == 'iNFT']['F1 score (iNFT)']\n",
    "    row.append(f'{lst.mean():.2f}' + u' \\u00B1 ' + f'{lst.std():.2f}')\n",
    "    \n",
    "    lst = em_df[em_df.label == 'iNFT']['F1 score (iNFT)']\n",
    "    row.append(f'{lst.mean():.2f}' + u' \\u00B1 ' + f'{lst.std():.2f}')\n",
    "    \n",
    "    # Macro F1 scores (averages).\n",
    "    lst = val_df[val_df.label == 'all']['macro F1 score']\n",
    "    row.append(f'{lst.mean():.2f}' + u' \\u00B1 ' + f'{lst.std():.2f}')\n",
    "    \n",
    "    lst = test_df[test_df.label == 'all']['macro F1 score']\n",
    "    row.append(f'{lst.mean():.2f}' + u' \\u00B1 ' + f'{lst.std():.2f}')\n",
    "    \n",
    "    lst = em_df[em_df.label == 'all']['macro F1 score']\n",
    "    row.append(f'{lst.mean():.2f}' + u' \\u00B1 ' + f'{lst.std():.2f}')\n",
    "    \n",
    "    perf_df.append(row)\n",
    "\n",
    "perf_df = DataFrame(\n",
    "    perf_df, \n",
    "    columns=['', 'Pre-NFT F1 Score', '', '', 'iNFT F1 Score', '', '', \n",
    "             'Macro F1 Score', '', '']\n",
    ")\n",
    "perf_df.to_csv(join(save_dir, 'annotator-model-results.csv'), \n",
    "               float_format='%11.2f', index=False)\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167741b",
   "metadata": {},
   "source": [
    "## Model Result Interactive\n",
    "Allow viewing results with different datasets and metrics for the annotator models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c32664",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Bar plot interactive.\n",
    "model_bars(\n",
    "    model_results, \n",
    "    datasets=datasets, \n",
    "    models=models,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32fe56",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "Allow selecting a dataset and model to show its confusion matrix.\n",
    "\n",
    "Add a toggle for showing version with Pre-NFT and iNFT grouped into a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8280f18",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Confusion matrix interactive.\n",
    "confusion_matrices(model_results, datasets=datasets, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ecfce1",
   "metadata": {},
   "source": [
    "## Emory Hold-out Dataset Table\n",
    "For each ROI in the Emory hold-out dataset, add the metadata to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef592c2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "holdout_df = []\n",
    "\n",
    "# ROI metadata\n",
    "rois_df = read_csv('csvs/labeled-rois.csv')\n",
    "rois_df = rois_df[rois_df.roi_group == 'test-roi']\n",
    "\n",
    "# Case metadata\n",
    "cases_df = read_csv('csvs/cases.csv')\n",
    "cases_df = cases_df[cases_df.case.isin(rois_df.case.unique())]\n",
    "\n",
    "i = 1\n",
    "\n",
    "for _, r in rois_df.iterrows():    \n",
    "    # Find case metadata.\n",
    "    case_meta = cases_df[cases_df.case == r.case].iloc[0]\n",
    "    \n",
    "    # Calculate the width and height of ROI in microns.\n",
    "    tile_metadata = get_tile_metadata(gc, r.wsi_id)\n",
    "    \n",
    "    w = int(tile_metadata['mm_x'] * r.roi_width * 1000)\n",
    "    h = int(tile_metadata['mm_y'] * r.roi_height * 1000)\n",
    "    \n",
    "    holdout_df.append([\n",
    "        i,\n",
    "        r.case,\n",
    "        r.wsi_name,\n",
    "        r.Braak_stage,\n",
    "        r.region,\n",
    "        w,\n",
    "        h,\n",
    "        case_meta.Primary_NP_Dx,\n",
    "    ])\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Compile into table to save.\n",
    "holdout_df = DataFrame(\n",
    "    holdout_df,\n",
    "    columns=['ROI #', 'Case', 'WSI Filename', 'Braak Stage', 'Brain Region',\n",
    "             'Width (microns)', 'Height (microns)', \n",
    "             'Primary Neuropathology Diagnosis'\n",
    "            ]\n",
    ")\n",
    "\n",
    "holdout_df.to_csv(join(save_dir, 'Emory-holdout-metadata.csv'), index=False)\n",
    "holdout_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dfac0",
   "metadata": {},
   "source": [
    "## Average Size of ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07642c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the average size of ROIs (in pixels) used in annotator models.\n",
    "rois_df = read_csv('csvs/labeled-rois.csv')\n",
    "rois_df = rois_df[rois_df.roi_group.isin((\n",
    "    'ROIv2', 'ROIv1'\n",
    "))]\n",
    "\n",
    "roi_widths = rois_df.roi_width.astype(int)\n",
    "roi_heights = rois_df.roi_height.astype(int)\n",
    "\n",
    "print(\n",
    "    'Average size of ROIs in pixels (width x height): '\n",
    "    f'{np.mean(roi_widths):.0f} x {np.mean(roi_heights):.0f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421546bb",
   "metadata": {},
   "source": [
    "## Average Size of Large ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b864f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the average size of ROIs (in pixels) used in annotator models.\n",
    "rois_df = read_csv('../data/datasets/model-assisted-labeling/rois.csv')\n",
    "rois_df = rois_df[rois_df.group == 'ROIv3']\n",
    "\n",
    "# roi_widths = rois_df.roi_width.astype(int)\n",
    "# roi_heights = rois_df.roi_height.astype(int)\n",
    "\n",
    "# print(\n",
    "#     'Average size of ROIs in pixels (width x height): '\n",
    "#     f'{np.mean(roi_widths):.0f} x {np.mean(roi_heights):.0f}'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
