{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f2fe4d",
   "metadata": {},
   "source": [
    "# Results: WSI Inference Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422fe55",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pandas import read_csv, DataFrame\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from colorama import Fore, Style\n",
    "from multiprocessing import Pool\n",
    "import networkx as nx\n",
    "from libpysal import weights\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, Dropdown\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import join, isdir, isfile\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "from nft_helpers.utils import get_filename, load_yaml, imread\n",
    "from nft_helpers.girder_dsa import get_tile_metadata, login\n",
    "from nft_helpers.roi_utils import read_roi_txt_file\n",
    "\n",
    "# Prepare parameters.\n",
    "cf = load_yaml()\n",
    "gc = login(join(cf.dsaURL, 'api/v1'), username=cf.user, password=cf.password)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "save_dir = join(cf.datadir, 'results/wsi-inference')\n",
    "makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "COLORS = [f'#{color}' for color in cf.colors]\n",
    "LINESTYLES = ['solid', 'dotted', 'dashed', 'dashdot', (5, (10, 3))]\n",
    "\n",
    "\n",
    "def plot_cm(cm: np.array, labels: List[str], title: str = '', \n",
    "            figsize: Tuple[int, int] = (4, 4)):\n",
    "    \"\"\"Plot confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix with rows are true and columns are predictions.\n",
    "        labels: Labels of the confusion matrix.\n",
    "        title: Title of plot.\n",
    "        figsize: Size of figure.\n",
    "        \n",
    "    \"\"\"\n",
    "    cm = DataFrame(cm, index=labels, columns=labels)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(\n",
    "        cm, cmap='viridis', annot=True, cbar=False, fmt=\".0f\", square=True, \n",
    "        linewidths=1, linecolor='black', annot_kws={\"size\": 18}\n",
    "    )\n",
    "    ax.xaxis.set_ticks_position(\"none\")\n",
    "    ax.yaxis.set_ticks_position(\"none\")\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=14)\n",
    "    \n",
    "    plt.ylabel('True', fontsize=18, fontweight='bold')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=14)\n",
    "    \n",
    "    plt.xlabel('Predicted', fontsize=18, fontweight='bold')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_tri_heatmap(data: np.array, labels: list, figsize: (int, int) = (7,7), \n",
    "                     title: str = None, save_fp: str = None, **kwargs: dict\n",
    "                    ) -> np.array:\n",
    "    \"\"\"Create a correlation heatmap from an array, only plotting the bottom half\n",
    "    triangle of the heatmap. \n",
    "    \n",
    "    Args:\n",
    "        data: Data to plot, the data is assumed to be symetrical and only the \n",
    "            bottom left triangle of the heatmap will be shown.\n",
    "        labels: Labels on both axis, ordered from top to bottom and left to \n",
    "            right.\n",
    "        figsize: (width, height) of figure.\n",
    "        title: Title of figure.\n",
    "        save_fp: Filepath to save figure to.\n",
    "        kwargs: Keyword arguments to pass to seaborn.heatmap()\n",
    "       \n",
    "    Returns:\n",
    "        The input data array.\n",
    "        \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    mask = np.triu(np.ones_like(data), k=1)\n",
    "    ax = sns.heatmap(data, annot=True, mask=mask, xticklabels=labels, \n",
    "                     yticklabels=labels, ax=ax, **kwargs)\n",
    "    ax.set_xticks(ax.get_xticks(), labels, size=16)\n",
    "    ax.set_yticks(ax.get_yticks(), labels, size=16, rotation=360)\n",
    "    ax.set_facecolor('k')\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=18, weight='bold')\n",
    "        \n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=16)\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21c8f3",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Parameters to apply in the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 64  # seed random state for reproducibility\n",
    "RFE_FEATURES = 20  # number of features to use for random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08405a6",
   "metadata": {},
   "source": [
    "## WSI Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1bb700",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Map wsi filename to DSA id.\n",
    "wsis = read_csv('csvs/wsis.csv')\n",
    "wsi_ids = {get_filename(r.wsi_name): r.wsi_id for _, r in wsis.iterrows()}\n",
    "\n",
    "# Compile time it took to run WSI inference - use a single cohort.\n",
    "inf_dir = join(cf.datadir, 'wsi-inference/results/inference-cohort-1')\n",
    "\n",
    "# Compile the results as dataframe.\n",
    "wsi_inf_df = []\n",
    "\n",
    "for log_fp in tqdm(glob(join(inf_dir, 'logs/*.txt'))):\n",
    "    # WSI filename.\n",
    "    fn = get_filename(log_fp)\n",
    "    \n",
    "    with open(log_fp, 'r') as fh:\n",
    "        time_logs = fh.readlines()\n",
    "        \n",
    "    # Get GPU info and times.\n",
    "    gpus = None\n",
    "    gpu_flag = False\n",
    "    time_flag = False\n",
    "    times = {}\n",
    "\n",
    "    # Get GPU info.\n",
    "    for ln in time_logs:\n",
    "        if ln.startswith('GPU'):\n",
    "            gpu_flag = True\n",
    "        elif gpu_flag:\n",
    "            gpus = ln.strip()\n",
    "            gpu_flag = False\n",
    "        elif ln.startswith('Times'):\n",
    "            time_flag = True\n",
    "        elif time_flag:\n",
    "            time, seconds = ln.strip().split(': ')\n",
    "            times[time] = int(float(seconds) / 60)\n",
    "            \n",
    "    # Get info on the WSI\n",
    "    wsi_id = wsi_ids[fn]\n",
    "    wsi_metadata = get_tile_metadata(gc, wsi_id)\n",
    "    w, h = wsi_metadata['sizeX'], wsi_metadata['sizeY']\n",
    "    \n",
    "    # Read the tissue mask.\n",
    "    mask = imread(join(\n",
    "        cf.datadir, 'wsi-inference/tissue-masks/masks', fn + '.png'\n",
    "    ), grayscale=True)\n",
    "    \n",
    "    # Scale factor of the low res tissue mask.\n",
    "    sf = w / mask.shape[1]\n",
    "    \n",
    "    # Get the number of tissue pixels as full resolution.\n",
    "    num_pos = np.count_nonzero(mask) * (sf * sf)\n",
    "    \n",
    "    # Convert to area in millimeters squared.\n",
    "    tissue_area = num_pos * (wsi_metadata['mm_x'] * wsi_metadata['mm_y'])\n",
    "    wsi_area = (w * h) * (wsi_metadata['mm_x'] * wsi_metadata['mm_y'])\n",
    "    \n",
    "    # Read the prediction info - mainly how many annotations there were.\n",
    "    preds = read_roi_txt_file(join(inf_dir, 'inference', fn + '.txt'))\n",
    "    \n",
    "    # Add all the info.\n",
    "    wsi_inf_df.append([\n",
    "        fn, wsi_id, gpus, times['Tiling'], times['Predicting'], \n",
    "        times['Merging predictions'], times['Cleaning up'], times['Total time'],\n",
    "        wsi_area, tissue_area, len(preds)\n",
    "    ])\n",
    "    \n",
    "wsi_inf_df = DataFrame(wsi_inf_df, columns=[\n",
    "    'wsi_name', 'wsi_id', 'GPU', 'Tiling', 'Predicting', \n",
    "    'Merging predictions', 'Clean up', 'Total', 'WSI Area (mm x mm)', \n",
    "    'Tissue Area (mm x mm)', '# of Predictions'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b42df5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Print some information on the numbers.\n",
    "print(f'Slowest WSI inference time: {wsi_inf_df.Total.max()} minutes.')\n",
    "print(f'Fastest WSI inference time: {wsi_inf_df.Total.min()} minutes.')\n",
    "\n",
    "# Averages, by GPU.\n",
    "print('\\nAverages of total times:')\n",
    "print(f'  (all) {wsi_inf_df.Total.mean():.2f} ± ' + \\\n",
    "      f'{wsi_inf_df.Total.std():.2f} minutes.')\n",
    "\n",
    "gpu_df = wsi_inf_df[wsi_inf_df.GPU == 'NVIDIA RTX A4500']\n",
    "print(f'  (A4500) {gpu_df.Total.mean():.2f} ± ' + \\\n",
    "      f'{gpu_df.Total.std():.2f} minutes.')\n",
    "\n",
    "gpu_df = wsi_inf_df[wsi_inf_df.GPU == 'NVIDIA RTX A5000']\n",
    "print(f'  (A5000) {gpu_df.Total.mean():.2f} ± ' + \\\n",
    "      f'{gpu_df.Total.std():.2f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea3bec",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot scatter / line plots of the data.\n",
    "wsi_inf_df = wsi_inf_df.sort_values(by='Total')\n",
    "\n",
    "\n",
    "def save_fig(fp: str, close: bool = True):\n",
    "    \"\"\"Save matplotlib figure.\n",
    "    \n",
    "    Args:\n",
    "        fp: Filepath to save figure to.\n",
    "        close: If True, close the figure so it does not show.\n",
    "        \n",
    "    \"\"\"\n",
    "    plt.savefig(fp, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    if close:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_times(\n",
    "    df: DataFrame, cols: list, figsize: Tuple[int, int] = (10, 5), \n",
    "    ylabel: str = 'Times (minutes)', xlabel: str = 'WSIs', title: str = None\n",
    "):\n",
    "    \"\"\"Plot time scatter / line figures.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe.\n",
    "        cols: List of columns to use to plot in y-axis.\n",
    "        figsize: Size of figure.\n",
    "        ylabel: Label on the y axis.\n",
    "        xlabel: Label on the x axis.\n",
    "        title: Figure title.\n",
    "        \n",
    "    Return:\n",
    "        Figure axis.\n",
    "        \n",
    "    \"\"\"    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    for i, t in enumerate(cols):\n",
    "        y = df[t].tolist()\n",
    "        x = np.arange(1, len(y)+1)\n",
    "\n",
    "        plt.plot(x, y, c=COLORS[i], linestyle=LINESTYLES[i])\n",
    "        \n",
    "    plt.xlim([0, len(df)])\n",
    "    plt.legend(cols, fontsize=12)\n",
    "    \n",
    "    plt.ylabel(ylabel, fontsize=18, fontweight='bold')\n",
    "    plt.xlabel(xlabel, fontsize=18, fontweight='bold')\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=18, fontweight='bold')\n",
    "        \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(3)\n",
    "    ax.spines['left'].set_linewidth(3)\n",
    "    ax.spines['right'].set_linewidth(3)\n",
    "    ax.tick_params(axis='both', which='both', direction='out', length=10, \n",
    "                   width=3)\n",
    "    plt.yticks(fontweight='bold', fontsize=16)\n",
    "    plt.xticks(fontweight='bold', fontsize=16)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "times = ['Total', 'Tiling', 'Predicting', 'Merging predictions', 'Clean up']\n",
    "ax = plot_times(wsi_inf_df, cols=times)\n",
    "plt.show()\n",
    "save_fig(join(save_dir, 'wsi-inference-times.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f0954",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation of the different times with different features.\n",
    "print('Pearson correlation with tissue area.')\n",
    "for t in times:\n",
    "    p = pearsonr(wsi_inf_df[t].tolist(), wsi_inf_df['Tissue Area (mm x mm)'])[1]\n",
    "    \n",
    "    print(f'  {t} p-value = {p:.4f}')\n",
    "    \n",
    "print('\\nPearson correlation with number of predictions.')\n",
    "for t in times:\n",
    "    p = pearsonr(wsi_inf_df[t].tolist(), wsi_inf_df['# of Predictions'])[1]\n",
    "    \n",
    "    print(f'  {t} p-value = {p:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b1956",
   "metadata": {},
   "source": [
    "## Load Imaging Features for Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04524d0d",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each case - create a feature vector and save as dataframe.\n",
    "case_fts_fp = join(save_dir, 'inference-features.csv')\n",
    "inf_dir = join(cf.datadir, 'wsi-inference')\n",
    "\n",
    "cases = read_csv('csvs/cases.csv')\n",
    "wsis = read_csv('csvs/wsis.csv')\n",
    "\n",
    "# WSIs in the three cohorts.\n",
    "wsis = wsis[wsis.cohort.isin((\n",
    "    'Inference-Cohort-1', 'Inference-Cohort-2', 'External-Cohort'\n",
    "))]\n",
    "\n",
    "regions = ['Hippocampus', 'Amygdala', 'Temporal cortex', 'Occipital cortex']\n",
    "stages = [0, 1, 2, 3, 4, 5, 6]\n",
    "dataset_map = {\n",
    "    'Inference-Cohort-1': 'train', 'Inference-Cohort-2': 'Emory test',\n",
    "    'External-Cohort': 'UC Davis test'\n",
    "}\n",
    "\n",
    "# Radius in microns used to calculate average clustering coefficient.\n",
    "radii = np.arange(150, 600, 50)\n",
    "\n",
    "\n",
    "def extract_case_features(case, fov_dir):\n",
    "    \"\"\"Extract a set of case features.\"\"\"\n",
    "    # WSIs in this case.\n",
    "    case_wsis = wsis[wsis.case == case]\n",
    "    \n",
    "    # Case metadata.\n",
    "    case_metadata = cases[cases.case == case].iloc[0]\n",
    "    \n",
    "    # Some cases have two hippocampuse slides - take only one.\n",
    "    if len(case_wsis) != 4:\n",
    "        # I know that these cases all have a right hippocampus slide that we \n",
    "        # ignore (e.g. take the other one).\n",
    "        case_wsis = case_wsis[case_wsis.region != 'Right hippocampus']\n",
    "        \n",
    "    # Rename hippocampus slides to a single unified name.\n",
    "    case_wsis = case_wsis.replace({'Left hippocampus': 'Hippocampus', \n",
    "                                   'Right hippocampus': 'Hippocampus'})\n",
    "    case_wsis = case_wsis.sort_values(by='region')\n",
    "    \n",
    "    # Get dataset this case belongs to.\n",
    "    dataset = dataset_map[case_metadata.cohort]\n",
    "    \n",
    "    # Braak stage - ground truth.\n",
    "    stage = case_metadata.Braak_stage\n",
    "    \n",
    "    # For cases with intermediate Braak stage 1-2, convert it to 2.\n",
    "    stage = 2 if stage == '1-2' else int(stage)\n",
    "        \n",
    "    # Add age, for calculation bind the 90+ to 90.\n",
    "    age = case_metadata.age_at_death\n",
    "    \n",
    "    if age == '90+':\n",
    "        age = 90\n",
    "    else:\n",
    "        age = int(age)\n",
    "        \n",
    "    abc = case_metadata.ABC\n",
    "    \n",
    "    try:\n",
    "        abc = int(abc)\n",
    "    except:\n",
    "        abc = -1\n",
    "        \n",
    "    # Add demographics.\n",
    "    case_fts = [\n",
    "        dataset, case, stage, age, 0 if case_metadata.sex == 'female' else 1,\n",
    "        abc\n",
    "    ]\n",
    "    \n",
    "    for region in regions:\n",
    "        # Get region WSI.\n",
    "        r = case_wsis[case_wsis.region == region].iloc[0]\n",
    "        fn = get_filename(r.wsi_name)\n",
    "        \n",
    "        # Get tile metadata.\n",
    "        img_metadata = get_tile_metadata(gc, r.wsi_id)\n",
    "        \n",
    "        # FOV size in pixels for this WSI, FOV is 4mm^2 by area and it is a \n",
    "        # square FOV.\n",
    "        fov_w = int(2 / img_metadata['mm_x'])\n",
    "        fov_h = int(2 / img_metadata['mm_y'])\n",
    "        \n",
    "        # Calculate the radii in pixels.\n",
    "        px_radii = [int(r / 1000 / img_metadata['mm_x']) for r in radii]\n",
    "        \n",
    "        # Get tissue area from the tissue mask.\n",
    "        mask = imread(join(inf_dir, f'tissue-masks/masks/{fn}.png'))\n",
    "        \n",
    "        # Get the denominator as the amount of tissue in mm x mm.\n",
    "        h, w = mask.shape[:2]\n",
    "        \n",
    "        # Pixel area scale factor (low res -> high res)\n",
    "        sf = (img_metadata['sizeY'] / h) * (img_metadata['sizeX'] / w)\n",
    "        \n",
    "        # Convert to scale factor in mm^2\n",
    "        sf *= img_metadata['mm_x'] * img_metadata['mm_y']\n",
    "\n",
    "        # Denominator is the mm^2 area that contains tissue.\n",
    "        den = np.count_nonzero(mask) * sf\n",
    "        \n",
    "        # Read the NFTs dectected.\n",
    "        preds = read_roi_txt_file(join(\n",
    "            inf_dir, f'results/{r.cohort.lower()}-additional-rois/inference/'\n",
    "            f'{fn}.txt'\n",
    "        ))\n",
    "        \n",
    "        if not len(preds):\n",
    "            raise Exception('No predictions found, logic not found for this.')\n",
    "        \n",
    "        # Add density of Pre-NFTs & iNFTs in the tissue.\n",
    "        case_fts.append(len(preds[preds[:, 0] == 0]) / den)\n",
    "        case_fts.append(len(preds[preds[:, 0] == 1]) / den)\n",
    "        \n",
    "        # Read the FOV info info or calculate them.\n",
    "        wsi_fov_dir = join(fov_dir, fn)\n",
    "        \n",
    "        if isdir(wsi_fov_dir):\n",
    "            highest_fov = {\n",
    "                0: read_csv(join(wsi_fov_dir, '0.csv')),\n",
    "                1: read_csv(join(wsi_fov_dir, '1.csv'))\n",
    "            }\n",
    "        else:\n",
    "            makedirs(wsi_fov_dir, exist_ok=True)\n",
    "            \n",
    "            # Find the FOV with the highest density of each type NFT\n",
    "            # Convert preds to a geodataframe\n",
    "            geopreds = []\n",
    "\n",
    "            for pred in preds:\n",
    "                label, x1, y1, x2, y2 = pred[:5]\n",
    "\n",
    "                geopreds.append([\n",
    "                    label, x1, y1, x2, y2, Point((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "                ])\n",
    "\n",
    "            geopreds = GeoDataFrame(\n",
    "                geopreds, columns=['label', 'x1', 'y1', 'x2', 'y2', 'geometry']\n",
    "            )\n",
    "\n",
    "            # Check FOVs with some overlap to catch highest FOV.\n",
    "            xys = []\n",
    "\n",
    "            for x in range(0, img_metadata['sizeX'], int(fov_w / 2)):\n",
    "                for y in range(0, img_metadata['sizeY'], int(fov_h / 2)):\n",
    "                    xys.append([x, y])\n",
    "\n",
    "            # loop for each class\n",
    "            highest_fov = {0: None, 1: None}\n",
    "\n",
    "            for cls in (0, 1):\n",
    "                highest_within = 0\n",
    "                \n",
    "                for xy in xys:\n",
    "                    x, y = xy\n",
    "\n",
    "                    # Create the FOV polygon.\n",
    "                    x1, y1, x2, y2 = x, y, x + fov_w, y + fov_h\n",
    "\n",
    "                    fov = corners_to_polygon(x1, y1, x2, y2)\n",
    "\n",
    "                    # Calculate how many points are in the FOV\n",
    "                    within = geopreds[geopreds.within(fov)]\n",
    "                    within = within[within.label == cls]\n",
    "\n",
    "                    if len(within) > highest_within:\n",
    "                        highest_fov[cls] = within.copy()\n",
    "                        highest_within = len(within)\n",
    "                        \n",
    "                if highest_fov[cls] is None:\n",
    "                    highest_fov[cls] = DataFrame(\n",
    "                        [],\n",
    "                        columns=['label', 'x1', 'y1', 'x2', 'y2', 'geometry']\n",
    "                    )\n",
    "                    \n",
    "                highest_fov[cls].to_csv(\n",
    "                    join(wsi_fov_dir, f'{cls}.csv'), index=False\n",
    "                )\n",
    "                        \n",
    "        # Add number of objects in the most populated FOV.\n",
    "        case_fts.append(len(highest_fov[0]))\n",
    "        case_fts.append(len(highest_fov[1]))\n",
    "            \n",
    "        # Calculate average clustering cofficient for different radii.\n",
    "        for cls in (0, 1):\n",
    "            coordinates = []\n",
    "\n",
    "            for _, r in highest_fov[cls].iterrows():\n",
    "                coordinates.append([(r.x1 + r.x2) / 2, (r.y1 + r.y2) / 2])\n",
    "            \n",
    "            coordinates = np.array(coordinates)\n",
    "            \n",
    "            if len(coordinates):\n",
    "                for r in px_radii:\n",
    "                    # https://networkx.org/documentation/stable/auto_examples/geospatial/plot_points.html\n",
    "                    dist = weights.DistanceBand.from_array(\n",
    "                        coordinates, threshold=r, silence_warnings=True\n",
    "                    )\n",
    "\n",
    "                    dist_graph = dist.to_networkx()\n",
    "\n",
    "                    # Calculate average clustering of the graph.\n",
    "                    mean_coef = nx.average_clustering(dist_graph)\n",
    "\n",
    "                    case_fts.append(nx.average_clustering(dist_graph))\n",
    "            else:\n",
    "                case_fts.append(0)\n",
    "        \n",
    "    return case_fts\n",
    "\n",
    "\n",
    "# Create location to save FOV files\n",
    "fov_dir = join(save_dir, 'fovs')\n",
    "makedirs(fov_dir, exist_ok=True)\n",
    "\n",
    "# Features list: split imaging featurse from others.\n",
    "cols = [\n",
    "        'dataset', 'case', 'stage', 'age', 'sex', 'ABC'\n",
    "    ]\n",
    "\n",
    "# Add features\n",
    "img_features = []\n",
    "\n",
    "for region in regions:\n",
    "    img_features.append(f'Pre-NFT density ({region})')\n",
    "    img_features.append(f'iNFT density ({region})')\n",
    "\n",
    "    img_features.append(f'Pre-NFT FOV count ({region})')\n",
    "\n",
    "    img_features.append(f'iNFT FOV count ({region})')\n",
    "\n",
    "    for lbl in ('Pre-NFT', 'iNFT'):\n",
    "        for r in radii:\n",
    "            img_features.append(f'{lbl} Clustering Coef (r={r}, {region})')\n",
    "\n",
    "# Load data from file or create it.\n",
    "if isfile(case_fts_fp):\n",
    "    data = read_csv(case_fts_fp).fillna(0)\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "    with Pool(20) as pool:\n",
    "        jobs = [\n",
    "            pool.apply_async(\n",
    "                func=extract_case_features,\n",
    "                args=(case, fov_dir,)\n",
    "            )\n",
    "            for case in wsis.case.unique()\n",
    "        ]\n",
    "\n",
    "        # Run the jobs, does not return anything but saves results.\n",
    "        for job in tqdm(jobs):\n",
    "            data.append(job.get())\n",
    "\n",
    "    data = DataFrame(data, columns=cols + img_features)\n",
    "    data.to_csv(join(save_dir, 'inference-features.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9058b",
   "metadata": {},
   "source": [
    "## Recursive Feature Selection\n",
    "Reduce the number of features used to train random forest classifier using recursive feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f366d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Recursive Feature Eliminitation (RFE) with Logistic Regression.\n",
    "train_data = data[data.dataset == 'Emory test']\n",
    "X_train = train_data[img_features].to_numpy()\n",
    "y_train = train_data.stage.tolist()\n",
    "\n",
    "model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Chooose the top 20 features - manually tested this number.\n",
    "rfe = RFE(model, n_features_to_select=RFE_FEATURES)\n",
    "\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "selected_features = np.array(img_features)[rfe.support_].tolist()\n",
    "\n",
    "# List of top 10 selected features.\n",
    "X_train_selected = train_data[selected_features].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb752b",
   "metadata": {},
   "source": [
    "## Random Forest Classifier: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09546a58",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tune Random Forest Classifier.\n",
    "params = {\n",
    "    'n_estimators': [\n",
    "        int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)\n",
    "    ],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [None] + [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create model (Random forest classifier).\n",
    "rfc = RandomForestClassifier(random_state=RANDOM_STATE, class_weight=None)\n",
    "\n",
    "# Search for best set of parameters, automatically fits the best parameters.\n",
    "gs_rfc = RandomizedSearchCV(\n",
    "    rfc, params, scoring='balanced_accuracy', cv=3, n_jobs=20, verbose=0, \n",
    "    random_state=RANDOM_STATE, n_iter=100\n",
    ")\n",
    "\n",
    "# Search for best hyperparameters.\n",
    "gs_rfc  = gs_rfc.fit(X_train_selected, y_train)\n",
    "\n",
    "# Best estimator.\n",
    "rfc = gs_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357b855",
   "metadata": {},
   "source": [
    "## Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf90dd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot the best features.\n",
    "importances = rfc.feature_importances_.tolist()\n",
    "importances, fts = (list(t) for t in zip(*sorted(zip(importances, selected_features))))\n",
    "\n",
    "ft_importances_df = []\n",
    "\n",
    "for imp, ft in zip(importances, fts):\n",
    "    ft_importances_df.append([ft, imp])\n",
    "    \n",
    "ft_importances_df = DataFrame(ft_importances_df, columns=['Feature', 'Importance'])\n",
    "ft_importances_df.to_csv(join(save_dir, 'feature-impotances.csv'), index=False)\n",
    "    \n",
    "# only plot the top 10 features\n",
    "n = len(importances)\n",
    "\n",
    "if n > 10:\n",
    "    importances = importances[n-10:]\n",
    "    fts = fts[n-10:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "y_pos = np.arange(len(fts))\n",
    "ax.barh(y_pos, importances)\n",
    "ax.set_yticks(y_pos, labels=fts)\n",
    "plt.ylabel('Feature', fontweight='bold', fontsize=16)\n",
    "plt.xlabel('Importance', fontweight='bold', fontsize=16)\n",
    "ax.tick_params(axis='both', which='both', direction='out', length=10, width=2)\n",
    "    \n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "plt.show()\n",
    "save_fig(join(save_dir, 'feature-importance.png'), close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3bcfd8",
   "metadata": {},
   "source": [
    "## Confusion Matrix (Emory-Train Cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5a756",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for Emory-Train cohort.\n",
    "stages = ['0', 'I', 'II', 'III', 'IV', 'V', 'VI']\n",
    "\n",
    "# Predict stages on the Emory train cohort (test for Braak stages).\n",
    "test_data = data[data.dataset == 'train']\n",
    "X_test = test_data[selected_features].to_numpy()\n",
    "y_test = test_data.stage.tolist()\n",
    "\n",
    "y_test_pred = rfc.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "k = cohen_kappa_score(y_test, y_test_pred, weights='quadratic')\n",
    "\n",
    "ax = plot_cm(cm, stages, title=f\"Emory Train Cohort (k={k:.2f})\")\n",
    "\n",
    "for i in range(len(stages)):\n",
    "    ax.add_patch(Rectangle((i, i), 1, 1, fill=False, edgecolor='green', lw=3, hatch='/'))\n",
    "plt.show()\n",
    "save_fig(join(save_dir, 'Braak-cm-Emory-train.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558ddfa",
   "metadata": {},
   "source": [
    "## Confusion Matrix (UC-Davis Cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9c4bf",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for UC-Davis cohort.\n",
    "davis_data = data[data.dataset == 'UC Davis test']\n",
    "X_davis = davis_data[selected_features].to_numpy()\n",
    "y_davis = davis_data.stage.tolist()\n",
    "\n",
    "y_davis_pred = rfc.predict(X_davis)\n",
    "\n",
    "cm = confusion_matrix(y_davis, y_davis_pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
    "k = cohen_kappa_score(y_davis, y_davis_pred, weights='quadratic', labels=[2, 3, 4, 5, 6])\n",
    "\n",
    "ax = plot_cm(cm, stages, title=f\"UC Davis Cohort (k={k:.2f})\")\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "for i in range(len(stages)):\n",
    "    ax.add_patch(Rectangle((i, i), 1, 1, fill=False, edgecolor='green', lw=3, hatch='/'))\n",
    "plt.show()\n",
    "save_fig(join(save_dir, 'Braak-cm-UC-Davis.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91768df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a big outlier - True = VI, predicted = 0 - identify this case.\n",
    "for i, r in davis_data.reset_index(drop=True).iterrows():\n",
    "    pred = y_davis_pred[i]\n",
    "    \n",
    "    if r.stage == 6 and pred == 0:\n",
    "        display(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d193d50",
   "metadata": {},
   "source": [
    "## Raters Agreement Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632bf28a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot paired Braak stage Cohen's kappa heatmap.\n",
    "wsis_df = read_csv('csvs/wsis.csv')\n",
    "wsis_df = wsis_df[\n",
    "    (wsis_df.cohort == 'Annotated-Cohort') & \\\n",
    "    (wsis_df.annotator_experience == 'expert')\n",
    "]\n",
    "\n",
    "experts = sorted(list(wsis_df.annotator.unique()))\n",
    "raters = experts + ['ML']\n",
    "\n",
    "# Add predicted Braak stage\n",
    "test_data = test_data.copy()\n",
    "test_data['ML'] = y_test_pred\n",
    "\n",
    "for expert in wsis_df.annotator.unique():\n",
    "    for i, case in enumerate(test_data.case.tolist()):\n",
    "        stage = wsis_df[(wsis_df.annotator == expert) & (wsis_df.case == case)].iloc[0].Braak_stage\n",
    "        \n",
    "        test_data.loc[i, expert] = int(stage)\n",
    "\n",
    "# Build the kappa array.\n",
    "kappa_hm = np.zeros((len(raters), len(raters)))\n",
    "\n",
    "for i, r1 in enumerate(raters):\n",
    "    for j, r2 in enumerate(raters):\n",
    "        k = cohen_kappa_score(\n",
    "            test_data[r1].tolist(), \n",
    "            test_data[r2].tolist(),\n",
    "            weights='quadratic',\n",
    "            labels=[0, 1, 2, 3, 4, 5, 6]\n",
    "        )\n",
    "        kappa_hm[i, j] = k\n",
    "        \n",
    "# Mask the top half.\n",
    "mask = np.triu(np.ones_like(kappa_hm))\n",
    "kappas = kappa_hm[mask == 0]\n",
    "\n",
    "kwargs = {'cmap': 'viridis', 'annot_kws': {\"size\":16}, 'linecolor': 'w', \n",
    "          'linewidths': 0}\n",
    "\n",
    "ax = plot_tri_heatmap(\n",
    "    kappa_hm, \n",
    "    ['E1', 'E2', 'E3', 'E4', 'E5', 'ML'], \n",
    "    figsize=(10,10), \n",
    "    title=f\"Braak Stage Agreement (k={np.mean(kappas):.2f} \" + u\"\\u00B1\" + \\\n",
    "          f' {np.std(kappas):.2f})',\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "plt.xlabel('Rater', fontsize=18, fontweight='bold')\n",
    "plt.ylabel('Rater', fontsize=18, fontweight='bold')\n",
    "plt.show()\n",
    "save_fig(join(save_dir, 'Braak-stage-agreement-hm.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377f9db",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Save features, including which features were used to train.\n",
    "ft_df = []\n",
    "\n",
    "for feature in selected_features:\n",
    "    ft_df.append([feature, 'Yes'])\n",
    "    \n",
    "for feature in img_features:\n",
    "    if feature not in selected_features:\n",
    "        ft_df.append([feature, 'No'])\n",
    "        \n",
    "ft_df = DataFrame(ft_df, columns=['Feature', 'Used for RF Training'])\n",
    "ft_df.to_csv(join(save_dir, 'features.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52d27c",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Braak stages from Raters against Ground Truth Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e1baf",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrices - for experts against the original Braak stage.\n",
    "iaa_df = read_csv('csvs/wsis.csv')\n",
    "iaa_df = iaa_df[\n",
    "    (iaa_df.cohort == 'Annotated-Cohort') & \\\n",
    "    (iaa_df.annotator_experience == 'expert')\n",
    "]\n",
    "\n",
    "iaa_cases = iaa_df.case.unique()\n",
    "\n",
    "true = []\n",
    "\n",
    "for case in iaa_cases:\n",
    "    stage = cases[cases.case == case].iloc[0].Braak_stage\n",
    "    \n",
    "    if stage == '1-2':\n",
    "        stage = 2\n",
    "    else:\n",
    "        stage = int(stage)\n",
    "\n",
    "    true.append(stage)\n",
    "\n",
    "\n",
    "def _plot_cm(annotator):\n",
    "    \"\"\"Plot confusion matrix of annotator stages to original stages.\"\"\"\n",
    "    pred = []\n",
    "    \n",
    "    for case in iaa_cases:\n",
    "        stage = iaa_df[(iaa_df.case == case) & (iaa_df.annotator == annotator)].iloc[0].Braak_stage\n",
    "        \n",
    "        pred.append(int(stage))\n",
    "        \n",
    "    k = cohen_kappa_score(true, pred, weights='quadratic', labels=[0, 1, 2, 3, 4, 5, 6])\n",
    "\n",
    "    # Confusion matrix.\n",
    "    cm = confusion_matrix(true, pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
    "    labels = ['0', '1', '2', '3', '4', '5', '6']\n",
    "    cm = DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "    # Plot the confusion matrix.\n",
    "    plt.figure(figsize=(4,4))\n",
    "    ax = sns.heatmap(\n",
    "        cm, cmap='viridis', annot=True, cbar=False, fmt=\".0f\", square=True, \n",
    "        linewidths=1, linecolor='black', annot_kws={\"size\": 18}\n",
    "    )\n",
    "    ax.xaxis.set_ticks_position(\"none\")\n",
    "    ax.yaxis.set_ticks_position(\"none\")\n",
    "\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=14)\n",
    "    plt.ylabel('True', fontsize=18, fontweight='bold')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=14)\n",
    "    plt.xlabel('Pred', fontsize=18, fontweight='bold')\n",
    "    plt.title(f\"Confusion Matrix for Annotator {annotator}\\nWeighted Cohen's Kappa: {k:.4f}\", fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "_ = interact(\n",
    "    _plot_cm, \n",
    "    annotator=Dropdown(options=sorted(list(iaa_df.annotator.unique())))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a8e68",
   "metadata": {},
   "source": [
    "## Compare Pre-NFT / iNFT Density Between Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867339e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "plot_df = []\n",
    "\n",
    "for _, r in data.iterrows():\n",
    "    # Add the density for this case.\n",
    "    row = [r.dataset, r.case, r.stage, r.age, r.sex, 'Pre-NFT']\n",
    "    row.extend([\n",
    "        r['Pre-NFT density (Hippocampus)'],\n",
    "        r['Pre-NFT density (Amygdala)'],\n",
    "        r['Pre-NFT density (Temporal cortex)'],\n",
    "        r['Pre-NFT density (Occipital cortex)'],\n",
    "        r['Pre-NFT FOV count (Hippocampus)'],\n",
    "        r['Pre-NFT FOV count (Amygdala)'],\n",
    "        r['Pre-NFT FOV count (Temporal cortex)'],\n",
    "        r['Pre-NFT FOV count (Occipital cortex)']\n",
    "    ])\n",
    "    plot_df.append(row)\n",
    "    \n",
    "    row = [r.dataset, r.case, r.stage, r.age, r.sex, 'iNFT']\n",
    "    row.extend([\n",
    "        r['iNFT density (Hippocampus)'],\n",
    "        r['iNFT density (Amygdala)'],\n",
    "        r['iNFT density (Temporal cortex)'],\n",
    "        r['iNFT density (Occipital cortex)'],\n",
    "        r['iNFT FOV count (Hippocampus)'],\n",
    "        r['iNFT FOV count (Amygdala)'],\n",
    "        r['iNFT FOV count (Temporal cortex)'],\n",
    "        r['iNFT FOV count (Occipital cortex)']\n",
    "    ])\n",
    "    plot_df.append(row)\n",
    "    \n",
    "plot_df = DataFrame(\n",
    "    plot_df, \n",
    "    columns=[\n",
    "        'dataset', 'case', 'stage', 'age_at_death', 'sex', 'label', \n",
    "        'Density (Hippocampus)', 'Density (Amygdala)', 'Density (Temporal)',\n",
    "        'Density (Occipital)', 'FOV count (Hippocampus)', \n",
    "        'FOV count (Amygdala)', 'FOV count (Temporal)', 'FOV count (occipital)'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def plot_grouped_plots(feature):\n",
    "    \"\"\"Plot grouped bar plot for a feature for Pre-NFT and iNFT\"\"\"\n",
    "    df = plot_df[plot_df.label == 'Pre-NFT']\n",
    "    \n",
    "    y_max = plot_df[feature].max()\n",
    "        \n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax1 = plt.subplot(121)\n",
    "    sns.boxplot(data=df, y=feature, x='stage', hue='dataset', ax=ax1, )\n",
    "#                 errorbar='se', capsize=.1, zorder=5)\n",
    "    plt.xlabel('Braak Stage', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel(feature, fontsize=16, fontweight='bold')\n",
    "    plt.xticks(fontweight='bold', fontsize=12)\n",
    "    plt.yticks(fontweight='bold', fontsize=12)\n",
    "    plt.title('Pre-NFT', fontsize=16, fontweight='bold', y=1.15)\n",
    "    plt.legend(ncol=3, fontsize=14, bbox_to_anchor=(0.55, 1.15), loc='upper center')\n",
    "    ax1.tick_params(axis='both', which='both', direction='out', length=10, \n",
    "                    width=2)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_linewidth(2)\n",
    "    ax1.spines['left'].set_linewidth(2)\n",
    "    \n",
    "    df = plot_df[plot_df.label == 'iNFT']\n",
    "    ax2 = plt.subplot(122, sharey=ax1)\n",
    "    sns.boxplot(data=df, y=feature, x='stage', hue='dataset', ax=ax2, )\n",
    "#                 errorbar='se', capsize=.1, zorder=5)\n",
    "    plt.ylabel(None)\n",
    "    plt.xlabel('Braak Stage', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(fontweight='bold', fontsize=12)\n",
    "    plt.yticks(fontweight='bold', fontsize=12)\n",
    "    plt.title('iNFT', fontsize=16, fontweight='bold', y=1.15)\n",
    "    ax2.tick_params(axis='both', which='both', direction='out', length=10, \n",
    "                    width=2)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['bottom'].set_linewidth(2)\n",
    "    ax2.spines['left'].set_linewidth(2)\n",
    "    ax2.get_legend().remove()\n",
    "    \n",
    "    save_fp = join(save_dir, f'{feature} bars.png')\n",
    "    \n",
    "    if not isfile(save_fp):\n",
    "        plt.savefig(save_fp, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "_ = interact(\n",
    "    plot_grouped_plots, \n",
    "    feature=Dropdown(options=[\n",
    "        'Density (Hippocampus)', 'Density (Amygdala)', 'Density (Temporal)',\n",
    "        'Density (Occipital)', 'FOV count (Hippocampus)', \n",
    "        'FOV count (Amygdala)', 'FOV count (Temporal)', 'FOV count (occipital)'\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c4a82d",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # For each stage group & class calculate statistical significance.\n",
    "# densities = [\n",
    "#     'Density (Hippocampus)', 'Density (Amygdala)', 'Density (Temporal)',\n",
    "#     'Density (Occipital)'\n",
    "# ]\n",
    "\n",
    "# results = 'Statistical signficance between cohorts in groups.\\n\\n'\n",
    "\n",
    "# cohorts = ['train', 'Emory test', 'UC Davis test']\n",
    "# stages = [2, 3, 4, 5, 6]\n",
    "\n",
    "# for density in densities:\n",
    "#     for stage in stages:\n",
    "#         results += f'{density} Braak stage {stage}:\\n'\n",
    "#         for lb in ['Pre-NFT', 'iNFT']:\n",
    "#             lb_df = plot_df[(plot_df.label == lb) & (plot_df.stage == stage)]\n",
    "\n",
    "#             results += f'  o {lb}\\n'\n",
    "\n",
    "#             # Calculate one-way ANOVA between the three groups.\n",
    "#             group1 = lb_df[lb_df.dataset == 'train'][density]\n",
    "#             group2 = lb_df[lb_df.dataset == 'Emory test'][density]\n",
    "#             group3 = lb_df[lb_df.dataset == 'UC Davis test'][density]\n",
    "\n",
    "#             s, p = f_oneway(group1, group2, group3)\n",
    "\n",
    "#             results += f'     - ANOVA: F={s:.4f}, p-value={p:.4f}\\n'\n",
    "\n",
    "#             # If the p-value is less then 0.05 then follow with post-hoc Tukey's\n",
    "#             # test to identify groups that are significant from each other.\n",
    "#             if p < 0.05:\n",
    "#                 m_comp = pairwise_tukeyhsd(endog=lb_df[density], groups=lb_df.dataset, alpha=0.05)\n",
    "                \n",
    "#                 # Convert to a dataframe.\n",
    "#                 m_comp = m_comp.summary().data\n",
    "#                 m_comp = DataFrame(m_comp[1:], columns=m_comp[0])\n",
    "                \n",
    "#                 # Add pairs that where different.\n",
    "#                 for _, r in m_comp[m_comp.reject].iterrows():\n",
    "#                     results += f\"     - {r.group1} & {r.group2} Pair Tukeys p-value={r['p-adj']:.4f}\\n\"\n",
    "\n",
    "# #                 print(density, lb)\n",
    "# #                 print(m_comp)\n",
    "# #                 print()\n",
    "# #                 for pair in combinations(cohorts, 2):\n",
    "# #                     cohort1, cohort2 = pair\n",
    "\n",
    "# #                     endog = lb_df[lb_df.dataset.isin((cohort1, cohort2))].reset_index(drop=True)\n",
    "# #                     m_comp = pairwise_tukeyhsd(endog=endog[density].to_numpy(), groups=endog['dataset'].to_numpy(), alpha=0.05)\n",
    "\n",
    "        \n",
    "#         results += '\\n'\n",
    "        \n",
    "#     results += '\\n\\n'\n",
    "#         # Calculate a t-test between groups.\n",
    "# #         for cohort_pair in combinations(cohorts, 2):\n",
    "# #             cohort1, cohort2 = cohort_pair\n",
    "            \n",
    "# #             s, p = ttest_ind(\n",
    "# #                 plot_df[\n",
    "# #                     (plot_df.dataset == cohort1) & (plot_df.label == lb)\n",
    "# #                 ][density],\n",
    "# #                 plot_df[\n",
    "# #                     (plot_df.dataset == cohort2) & (plot_df.label == lb)\n",
    "# #                 ][density]\n",
    "# #             )\n",
    "            \n",
    "# #             results += f'     - {cohort1} & {cohort2}, statistic = {s:.4f}, p-value = {p:.4f}\\n'\n",
    "                \n",
    "# print(results)\n",
    "\n",
    "# plot_df.dataset.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dadb48",
   "metadata": {},
   "source": [
    "## ML Model for Predicting ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive feature extraction.\n",
    "data_abc = data[data.ABC >= 0]\n",
    "\n",
    "train_data_abc = data_abc[data_abc.dataset == 'Emory test']\n",
    "X_train_abc = train_data_abc[img_features].to_numpy()\n",
    "y_train_abc = train_data_abc.ABC.tolist()\n",
    "\n",
    "model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Chooose the top 20 features - manually tested this number.\n",
    "rfe_abc = RFE(model, n_features_to_select=RFE_FEATURES)\n",
    "\n",
    "rfe_abc = rfe_abc.fit(X_train_abc, y_train_abc)\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "selected_features_abc = np.array(img_features)[rfe_abc.support_].tolist()\n",
    "\n",
    "# List of top 10 selected features.\n",
    "X_train_selected_abc = train_data_abc[selected_features_abc].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tune Random Forest Classifier.\n",
    "rfc_abc = RandomForestClassifier(random_state=RANDOM_STATE, class_weight=None)\n",
    "\n",
    "# Search for best set of parameters, automatically fits the best parameters.\n",
    "gs_rfc = RandomizedSearchCV(\n",
    "    rfc_abc, params, scoring='balanced_accuracy', cv=3, n_jobs=20, verbose=0, \n",
    "    random_state=RANDOM_STATE, n_iter=100\n",
    ")\n",
    "\n",
    "# Search for best hyperparameters.\n",
    "gs_rfc  = gs_rfc.fit(X_train_selected_abc, y_train_abc)\n",
    "\n",
    "# Best estimator.\n",
    "rfc_abc = gs_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best features.\n",
    "importances = rfc_abc.feature_importances_.tolist()\n",
    "importances, fts = (list(t) for t in zip(*sorted(zip(importances, selected_features))))\n",
    "\n",
    "ft_importances_df = []\n",
    "\n",
    "for imp, ft in zip(importances, fts):\n",
    "    ft_importances_df.append([ft, imp])\n",
    "    \n",
    "ft_importances_df = DataFrame(ft_importances_df, columns=['Feature', 'Importance'])\n",
    "    \n",
    "# only plot the top 10 features\n",
    "n = len(importances)\n",
    "\n",
    "if n > 10:\n",
    "    importances = importances[n-10:]\n",
    "    fts = fts[n-10:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "y_pos = np.arange(len(fts))\n",
    "ax.barh(y_pos, importances)\n",
    "ax.set_yticks(y_pos, labels=fts)\n",
    "plt.ylabel('Feature', fontweight='bold', fontsize=16)\n",
    "plt.xlabel('Importance', fontweight='bold', fontsize=16)\n",
    "ax.tick_params(axis='both', which='both', direction='out', length=10, width=2)\n",
    "    \n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Emory-Train cohort.\n",
    "abcs = ['0', '1', '2', '3']\n",
    "\n",
    "# Predict stages on the Emory train cohort (test for Braak stages).\n",
    "test_data_abc = data_abc[data_abc.dataset == 'train']\n",
    "X_test_abc = test_data_abc[selected_features_abc].to_numpy()\n",
    "y_test_abc = test_data_abc.ABC.tolist()\n",
    "\n",
    "y_test_pred_abc = rfc_abc.predict(X_test_abc)\n",
    "\n",
    "cm = confusion_matrix(y_test_abc, y_test_pred_abc)\n",
    "k = cohen_kappa_score(y_test_abc, y_test_pred_abc, weights='quadratic')\n",
    "\n",
    "ax = plot_cm(cm, abcs, title=f\"[ABC] Emory Train Cohort (k={k:.2f})\")\n",
    "\n",
    "for i in range(len(abcs)):\n",
    "    ax.add_patch(Rectangle((i, i), 1, 1, fill=False, edgecolor='green', lw=3, hatch='/'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
