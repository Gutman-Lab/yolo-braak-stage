{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4721928d",
   "metadata": {},
   "source": [
    "# Results: Model Assisted Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075bbf1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nft_helpers.plot import plot_bars\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv, DataFrame\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import join, isfile\n",
    "\n",
    "from nft_helpers import compile_model_results\n",
    "from nft_helpers.utils import load_yaml, im_to_txt_path\n",
    "from nft_helpers.interactive import model_bars, confusion_matrices\n",
    "from nft_helpers.yolov5.utils import read_yolo_label\n",
    "\n",
    "# Parameters\n",
    "cf = load_yaml()\n",
    "model_results = compile_model_results(join(cf.datadir, 'models'))\n",
    "\n",
    "kwargs = {\n",
    "    'errorbar': 'se', 'edgecolor': 'k', 'lw': 3, 'width': 0.5, 'errcolor': 'k', \n",
    "    'errwidth': 3, 'capsize': 0.25\n",
    "}\n",
    "\n",
    "# Location of model assisted labeling files.\n",
    "dataset_dir = join(cf.datadir, 'datasets/model-assisted-labeling')\n",
    "\n",
    "# Location to save results.\n",
    "save_dir = join(cf.datadir, 'results/model-assisted-labeling')\n",
    "makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "COLORS = [f'#{color}' for color in cf.colors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c24b42",
   "metadata": {},
   "source": [
    "## Models Trained with *n*-Model Agreement Labels\n",
    "Large dataset is trained with labels created from agreement of model predictions.\n",
    "\n",
    "Workflow:\n",
    "1. Train models on human-annotated datasets (annotator-specific models).\n",
    "2. Select a large dataset of ROIs without labels.\n",
    "3. Predict on these unlabeled ROIs with the annotator models\n",
    "4. Combine model predictions using *n*-consensus* voting\n",
    "5. Train models with now labeled datasets\n",
    "\n",
    "\\* The *n* can vary from as little as a single model to as stringent as all models must agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194ded8",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Interactive - consensus labeled models only.\n",
    "models = [\n",
    "    '1-models-consensus', '2-models-consensus', '3-models-consensus',\n",
    "    '4-models-consensus', '5-models-consensus', '6-models-consensus',\n",
    "    '7-models-consensus', '8-models-consensus'\n",
    "]\n",
    "\n",
    "datasets = ['val', 'test-roi', 'test-external-roi']\n",
    "\n",
    "model_bars(\n",
    "    model_results, \n",
    "    datasets=datasets, \n",
    "    models=models,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be1533",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Tailor the plot for the figure\n",
    "# Plotting the 8 model consensus models for the Emory test dataset and \n",
    "# showing the micro-F1 score.\n",
    "plot_df = model_results[\n",
    "    (model_results.label == 'all') & (model_results.dataset == 'test-roi')\n",
    "]\n",
    "\n",
    "# Format plot.\n",
    "kwargs['hatch'] = '/'\n",
    "kwargs['color'] = '#FFC107'\n",
    "''\n",
    "ax = plot_bars(\n",
    "    plot_df, \n",
    "    x_col='model', \n",
    "    y_col='macro F1 score',\n",
    "    order=[\n",
    "        '1-models-consensus', '2-models-consensus', '3-models-consensus',\n",
    "        '4-models-consensus', '5-models-consensus', '6-models-consensus',\n",
    "        '7-models-consensus', '8-models-consensus'\n",
    "    ],\n",
    "    x_tick_rotation=0,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, fmt='%.2f', padding=20)\n",
    "        \n",
    "ax.set_xticklabels(['1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "plt.xlabel('Number of Models in Consensus', fontsize=18)\n",
    "plt.title('Performance on Emory Holdout Dataset', fontsize=18, \n",
    "          fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.savefig(\n",
    "    join(save_dir, 'consensus-model-performance.png'), dpi=300, \n",
    "    bbox_inches='tight'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f59670",
   "metadata": {},
   "source": [
    "## Model Assisted Labeling\n",
    "Compare to 4-models-consensus, since this is where I started the model-assisted-labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5334e61",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Interactive for model assisted labeling models.\n",
    "models = [\n",
    "    'iteration1', 'iteration2', 'iteration3',\n",
    "     'iteration4', 'iteration5', 'iteration6', 'iteration7', 'iteration8',\n",
    "]\n",
    "\n",
    "datasets = ['test-roi', 'test-external-roi']\n",
    "\n",
    "model_bars(\n",
    "    model_results, \n",
    "    datasets=datasets, \n",
    "    models=models,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1a3c1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Save the plot of iterations.\n",
    "plot_df = model_results[\n",
    "    (model_results.label == 'Pre-NFT') & (model_results.dataset == 'test-roi')\n",
    "]\n",
    "\n",
    "# Format plot.\n",
    "kwargs['hatch'] = cf.hatches[1]\n",
    "kwargs['color'] = COLORS[1]\n",
    "''\n",
    "ax = plot_bars(\n",
    "    plot_df, \n",
    "    x_col='model', \n",
    "    y_col='F1 score (iNFT)',\n",
    "    order=[\n",
    "        'iteration1', 'iteration2', 'iteration3', 'iteration4', 'iteration5',\n",
    "        'iteration6', 'iteration7', 'iteration8', 'iteration8-cleaned-only'\n",
    "    ],\n",
    "    x_tick_rotation=0,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, fmt='%.2f', padding=20)\n",
    "        \n",
    "ax.set_xticklabels(['1', '2', '3', '4', '5', '6', '7', '8', '8c'])\n",
    "plt.xlabel('Iteration Number', fontsize=18)\n",
    "plt.ylabel('Pre-NFT F1 Score', fontsize=18)\n",
    "plt.title('Model-assisted Labeling Models', fontsize=18, \n",
    "          fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "# plt.savefig(\n",
    "#     join(save_dir, 'model-assisted-labeling.png'), dpi=300, \n",
    "#     bbox_inches='tight'\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfdd23",
   "metadata": {},
   "source": [
    "## Table {to be determined}: Iteration models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582037e2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create table just for the Emory test dataset.\n",
    "models = [\n",
    "    'iteration1', 'iteration2', 'iteration3', 'iteration4', 'iteration5', \n",
    "    'iteration6', 'iteration7', 'iteration8', 'iteration8-Amygdala',\n",
    "    'iteration8-Hippocampus', 'iteration8-Temporal', 'iteration8-Occipital',\n",
    "    'iteration8-cleaned-only', '2-models-consensus', \n",
    "]\n",
    "\n",
    "model_labels = [\n",
    "    'iter. 1', 'iter. 2', 'iter. 3', 'iter. 4', 'iter. 5', \n",
    "    'iter. 6', 'iter. 7', 'iter. 8', 'amygdala', 'hippocampus', \n",
    "    'temporal', 'occipital', 'QC ROIs', 'best consensus', \n",
    "]\n",
    "\n",
    "iteration_df = [[\n",
    "    '', 'Precision', 'Recall', 'F1 score', 'Precision', 'Recall', 'F1 Score',\n",
    "    'Macro F1-score'\n",
    "]]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    # Subset to this model only.\n",
    "    model_df = model_results[\n",
    "        (model_results.model == model) & (model_results.dataset == 'test-roi')\n",
    "    ]\n",
    "    \n",
    "    row = [model_labels[i]]\n",
    "    \n",
    "    # Pull out F1 score, precision and recall for both classes\n",
    "    for cls in ('Pre-NFT', 'iNFT'):\n",
    "        cls_df = model_df[model_df.label == cls]\n",
    "        \n",
    "        scr = cls_df[f'Precision ({cls})']\n",
    "        row.append(f'{scr.mean():.2f} ± {scr.std():.2f}')\n",
    "        \n",
    "        scr = cls_df[f'Recall ({cls})']\n",
    "        row.append(f'{scr.mean():.2f} ± {scr.std():.2f}')\n",
    "        \n",
    "        scr = cls_df[f'F1 score ({cls})']\n",
    "        row.append(f'{scr.mean():.2f} ± {scr.std():.2f}')\n",
    "        \n",
    "    # Add the micro score\n",
    "    cls_df = model_df[model_df.label == 'all']\n",
    "    scr = cls_df['macro F1 score']\n",
    "    row.append(f'{scr.mean():.2f} ± {scr.std():.2f}')\n",
    "    \n",
    "    iteration_df.append(row)\n",
    "        \n",
    "iteration_df = DataFrame(\n",
    "    iteration_df, \n",
    "    columns=['', 'Pre-NFT', '', '', 'iNFT', '', '', '']\n",
    ")\n",
    "iteration_df.to_csv(join(save_dir, 'iteration-models.csv'), index=False)\n",
    "iteration_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb3ddbc",
   "metadata": {},
   "source": [
    "## Model Assisted Labeling Time Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aeeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ROIs used during model assisted labeling.\n",
    "mal_df = read_csv('/workspace/data/datasets/model-assisted-labeling/model-assisted-labeling.csv')\n",
    "mal_df.iloc[0].fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8235b43",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot performance (micro F1-score) for the iteration models in the same plot\n",
    "# as time.\n",
    "xs = range(1, 9)\n",
    "\n",
    "# Get model performance\n",
    "y1 = []\n",
    "y2 = []\n",
    "\n",
    "for x in xs:  \n",
    "    y1.append([\n",
    "        model_results[\n",
    "            (model_results.model == f'iteration{x}') & \\\n",
    "            (model_results.dataset == 'test-roi') & \\\n",
    "            (model_results.label == 'all')\n",
    "        ]['micro F1 score'].mean()\n",
    "    ])\n",
    "    \n",
    "    with open(join(dataset_dir, f'timer-logs/{x}.txt'), 'r') as fh:\n",
    "        t = 0\n",
    "        \n",
    "        for line in fh.readlines():\n",
    "            if len(line):\n",
    "                t += int(line.strip())\n",
    "                \n",
    "        # Convert to minutes\n",
    "#         t = int(t / 60)\n",
    "        t = int(t)\n",
    "        \n",
    "    # Get ROIs for this iteration.\n",
    "    iter_df = mal_df[mal_df.iteration == x]\n",
    "    \n",
    "    n = 0\n",
    "    \n",
    "    for _, r in iter_df.iterrows():\n",
    "        label_fp = im_to_txt_path(r.fp)\n",
    "        \n",
    "        if isfile(label_fp):\n",
    "            boxes = read_yolo_label(label_fp)\n",
    "            \n",
    "            n += len(boxes)\n",
    "    \n",
    "    y2.append(t / n)\n",
    "        \n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(xs, y1, c=f'#{cf.colors[0]}', marker='o')\n",
    "plt.yticks(fontweight='bold', fontsize=16)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(xs, y2, c=f'#{cf.colors[1]}', marker='^')\n",
    "plt.yticks(fontweight='bold', fontsize=16)\n",
    "fig.tight_layout()\n",
    "ax1.legend(['Micro F1 Score'], bbox_to_anchor=(0.5, -0.32, 0.5, 0.5))\n",
    "ax2.legend(['Iteration Time'], loc='lower right')\n",
    "ax1.set_ylabel('Micro F1 Score', fontsize=18, fontweight='bold')\n",
    "ax2.set_ylabel('Time for each object (seconds)', fontsize=18, fontweight='bold')\n",
    "ax1.set_xlabel(\n",
    "    'Model-assisted Labeling Iteration', fontsize=18, fontweight='bold')\n",
    "plt.title('Model performance & Iteration Time', fontsize=18, fontweight='bold')\n",
    "ax1.spines['right'].set_linewidth(3)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['bottom'].set_linewidth(3)\n",
    "ax1.spines['left'].set_linewidth(3)\n",
    "ax2.spines['right'].set_linewidth(3)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['bottom'].set_linewidth(3)\n",
    "ax2.spines['left'].set_linewidth(3)\n",
    "ax1.tick_params(axis='both', which='both', direction='out', length=10, width=3)\n",
    "ax2.tick_params(axis='both', which='both', direction='out', length=10, width=3)\n",
    "plt.savefig(\n",
    "    join(save_dir, 'iteration-performance-and-time.png'), dpi=300, \n",
    "    bbox_inches='tight'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd0195",
   "metadata": {},
   "source": [
    "## Compare annotator models against best set of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3b36a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "models = [\n",
    "    'novice1', 'novice2', 'novice3', 'expert1', 'expert2', 'expert3', 'expert4',\n",
    "    'expert5', '2-models-consensus', '4-models-consensus', 'iteration8', \n",
    "    'iteration8-cleaned-only'\n",
    "]\n",
    "\n",
    "datasets3 = ['test-roi', 'test-external-roi']\n",
    "\n",
    "model_bars(\n",
    "    model_results, \n",
    "    datasets=datasets, \n",
    "    models=models,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eaeb7c",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df8ae5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Interactive of confusion matrices.\n",
    "datasets = ['test-roi', 'test-external-roi']\n",
    "models = sorted(list(model_results.model.unique()))\n",
    "\n",
    "confusion_matrices(model_results, datasets=datasets, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33b178",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c3879",
   "metadata": {},
   "source": [
    "## Model trained with only cleaned up ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54253cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'iteration8', 'iteration8-cleaned-only', '2-models-consensus'\n",
    "]\n",
    "\n",
    "datasets3 = ['test-roi', 'test-external-roi']\n",
    "\n",
    "model_bars(\n",
    "    model_results, \n",
    "    datasets=datasets, \n",
    "    models=models,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c9265",
   "metadata": {},
   "source": [
    "### Models Trained on Region Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcbcf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'iteration8', 'iteration8-Hippocampus', 'iteration8-Amygdala', \n",
    "    'iteration8-Temporal', 'iteration8-Occipital'\n",
    "]\n",
    "\n",
    "datasets3 = ['test-roi', 'test-external-roi']\n",
    "\n",
    "model_bars(\n",
    "    model_results, \n",
    "    datasets=datasets, \n",
    "    models=models,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b7f74",
   "metadata": {},
   "source": [
    "## Models with Additional Background ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'iteration8-cleaned-only', 'additional-background-rois'\n",
    "]\n",
    "\n",
    "datasets = ['val', 'test-roi', 'test-external-roi']\n",
    "\n",
    "model_bars(\n",
    "    model_results, \n",
    "    datasets=datasets, \n",
    "    models=models,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6099eea",
   "metadata": {},
   "source": [
    "## Choose Which Models to Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1fb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'iteration8-cleaned-only', 'additional-background-rois', 'inter-annotator-agreement'\n",
    "]\n",
    "\n",
    "datasets = ['val', 'test-roi', 'test-external-roi']\n",
    "\n",
    "model_bars(\n",
    "    model_results, \n",
    "    datasets=datasets, \n",
    "    models=models,\n",
    "    **kwargs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
